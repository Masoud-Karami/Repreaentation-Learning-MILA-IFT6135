## IFT6135 Representation Learning_sequential language models
Word Level Models
  # Penn Treebank
Convolutional Neural Network
----------------------------
  * simple ("vanilla") RNN (recurrent neural network)
      * "Vanilla" Stochastic Gradient Descent (SGD)
      * SGD with a learning rate schedule
      * Adam
  * RNN with a gating mecahnism, Gated Recurrent Units (GRUs)
  * Attention Module of a Transformer Network
  * Training Language Models (cross-entropy loss as its performance) and report perplexity (PPL), which is
the exponentiated average per-token NLL (over all tokens)
  - Detailed evaluation of trained models
  
more details and .py codes
- [Language modeling is the task of predicting the next word or character in a document](https://github.com/sebastianruder/NLPprogress/blob/master/english/language_modeling.md)
- [Compression of Recurrent Neural Networks for Efficient Language Modeling](https://arxiv.org/pdf/1902.02380.pdf)
- [Recent Trends in Deep Learning Based Natural Language Processing](https://arxiv.org/pdf/1708.02709.pdf)
- [Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/treebank.html)
