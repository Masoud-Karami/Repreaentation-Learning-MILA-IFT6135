{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use cuda:0\n",
      "Using downloaded and verified file: svhn/train_32x32.mat\n",
      "Using downloaded and verified file: svhn/test_32x32.mat\n",
      "------- EPOCH 0 --------\n",
      "Training example 100 / 1031. Loss: 38834.5625\n",
      "Training example 200 / 1031. Loss: 17966.80078125\n",
      "Training example 300 / 1031. Loss: 15262.626953125\n",
      "Training example 400 / 1031. Loss: 13869.8291015625\n",
      "Training example 500 / 1031. Loss: 13133.416015625\n",
      "Training example 600 / 1031. Loss: 12274.4892578125\n",
      "Training example 700 / 1031. Loss: 12025.474609375\n",
      "Training example 800 / 1031. Loss: 11558.083984375\n",
      "Training example 900 / 1031. Loss: 11418.11328125\n",
      "Training example 1000 / 1031. Loss: 11212.109375\n",
      "------- EPOCH 1 --------\n",
      "Training example 100 / 1031. Loss: 14517.662109375\n",
      "Training example 200 / 1031. Loss: 10933.8408203125\n",
      "Training example 300 / 1031. Loss: 10979.2841796875\n",
      "Training example 400 / 1031. Loss: 10837.2236328125\n",
      "Training example 500 / 1031. Loss: 10604.6171875\n",
      "Training example 600 / 1031. Loss: 10609.8515625\n",
      "Training example 700 / 1031. Loss: 10461.0\n",
      "Training example 800 / 1031. Loss: 10558.0625\n",
      "Training example 900 / 1031. Loss: 10398.0712890625\n",
      "Training example 1000 / 1031. Loss: 10330.2080078125\n",
      "------- EPOCH 2 --------\n",
      "Training example 100 / 1031. Loss: 13570.400390625\n",
      "Training example 200 / 1031. Loss: 10171.3876953125\n",
      "Training example 300 / 1031. Loss: 10029.1376953125\n",
      "Training example 400 / 1031. Loss: 9981.130859375\n",
      "Training example 500 / 1031. Loss: 10139.48828125\n",
      "Training example 600 / 1031. Loss: 10103.53125\n",
      "Training example 700 / 1031. Loss: 10071.0380859375\n",
      "Training example 800 / 1031. Loss: 10025.8828125\n",
      "Training example 900 / 1031. Loss: 9893.5302734375\n",
      "Training example 1000 / 1031. Loss: 10100.7900390625\n",
      "------- EPOCH 3 --------\n",
      "Training example 100 / 1031. Loss: 12789.0537109375\n",
      "Training example 200 / 1031. Loss: 9774.298828125\n",
      "Training example 300 / 1031. Loss: 9954.83984375\n",
      "Training example 400 / 1031. Loss: 9784.99609375\n",
      "Training example 500 / 1031. Loss: 9662.6767578125\n",
      "Training example 600 / 1031. Loss: 9595.7392578125\n",
      "Training example 700 / 1031. Loss: 9669.5517578125\n",
      "Training example 800 / 1031. Loss: 9465.4140625\n",
      "Training example 900 / 1031. Loss: 9495.5546875\n",
      "Training example 1000 / 1031. Loss: 9535.591796875\n",
      "------- EPOCH 4 --------\n",
      "Training example 100 / 1031. Loss: 12475.900390625\n",
      "Training example 200 / 1031. Loss: 9404.2333984375\n",
      "Training example 300 / 1031. Loss: 9390.8017578125\n",
      "Training example 400 / 1031. Loss: 9422.6279296875\n",
      "Training example 500 / 1031. Loss: 9275.4111328125\n",
      "Training example 600 / 1031. Loss: 9329.65234375\n",
      "Training example 700 / 1031. Loss: 9358.701171875\n",
      "Training example 800 / 1031. Loss: 9304.537109375\n",
      "Training example 900 / 1031. Loss: 9347.841796875\n",
      "Training example 1000 / 1031. Loss: 9260.8330078125\n",
      "------- EPOCH 5 --------\n",
      "Training example 100 / 1031. Loss: 12090.0625\n",
      "Training example 200 / 1031. Loss: 9221.6005859375\n",
      "Training example 300 / 1031. Loss: 9213.3203125\n",
      "Training example 400 / 1031. Loss: 9203.4033203125\n",
      "Training example 500 / 1031. Loss: 9020.390625\n",
      "Training example 600 / 1031. Loss: 9210.6240234375\n",
      "Training example 700 / 1031. Loss: 9169.4560546875\n",
      "Training example 800 / 1031. Loss: 9100.0771484375\n",
      "Training example 900 / 1031. Loss: 9120.4345703125\n",
      "Training example 1000 / 1031. Loss: 9080.3681640625\n",
      "------- EPOCH 6 --------\n",
      "Training example 100 / 1031. Loss: 11960.4755859375\n",
      "Training example 200 / 1031. Loss: 9024.6640625\n",
      "Training example 300 / 1031. Loss: 9015.1875\n",
      "Training example 400 / 1031. Loss: 8944.4697265625\n",
      "Training example 500 / 1031. Loss: 8909.6669921875\n",
      "Training example 600 / 1031. Loss: 9102.4248046875\n",
      "Training example 700 / 1031. Loss: 8932.2919921875\n",
      "Training example 800 / 1031. Loss: 9001.4560546875\n",
      "Training example 900 / 1031. Loss: 9079.4189453125\n",
      "Training example 1000 / 1031. Loss: 8872.4541015625\n",
      "------- EPOCH 7 --------\n",
      "Training example 100 / 1031. Loss: 11579.4755859375\n",
      "Training example 200 / 1031. Loss: 8954.177734375\n",
      "Training example 300 / 1031. Loss: 8988.8349609375\n",
      "Training example 400 / 1031. Loss: 8860.3662109375\n",
      "Training example 500 / 1031. Loss: 8880.291015625\n",
      "Training example 600 / 1031. Loss: 8872.099609375\n",
      "Training example 700 / 1031. Loss: 8952.1376953125\n",
      "Training example 800 / 1031. Loss: 8953.583984375\n",
      "Training example 900 / 1031. Loss: 8829.8134765625\n",
      "Training example 1000 / 1031. Loss: 8884.44921875\n",
      "------- EPOCH 8 --------\n",
      "Training example 100 / 1031. Loss: 11636.658203125\n",
      "Training example 200 / 1031. Loss: 8848.689453125\n",
      "Training example 300 / 1031. Loss: 8723.7646484375\n",
      "Training example 400 / 1031. Loss: 8741.5087890625\n",
      "Training example 500 / 1031. Loss: 8877.025390625\n",
      "Training example 600 / 1031. Loss: 8691.4384765625\n",
      "Training example 700 / 1031. Loss: 8859.3212890625\n",
      "Training example 800 / 1031. Loss: 8815.517578125\n",
      "Training example 900 / 1031. Loss: 8739.56640625\n",
      "Training example 1000 / 1031. Loss: 8829.3916015625\n",
      "------- EPOCH 9 --------\n",
      "Training example 100 / 1031. Loss: 11513.9892578125\n",
      "Training example 200 / 1031. Loss: 8651.685546875\n",
      "Training example 300 / 1031. Loss: 8649.2685546875\n",
      "Training example 400 / 1031. Loss: 8774.2314453125\n",
      "Training example 500 / 1031. Loss: 8829.5751953125\n",
      "Training example 600 / 1031. Loss: 8648.88671875\n",
      "Training example 700 / 1031. Loss: 8750.90234375\n",
      "Training example 800 / 1031. Loss: 8735.9755859375\n",
      "Training example 900 / 1031. Loss: 8731.5986328125\n",
      "Training example 1000 / 1031. Loss: 8807.556640625\n",
      "------- EPOCH 10 --------\n",
      "Training example 100 / 1031. Loss: 11286.0009765625\n",
      "Training example 200 / 1031. Loss: 8698.931640625\n",
      "Training example 300 / 1031. Loss: 8668.7890625\n",
      "Training example 400 / 1031. Loss: 8680.7578125\n",
      "Training example 500 / 1031. Loss: 8622.2626953125\n",
      "Training example 600 / 1031. Loss: 8681.474609375\n",
      "Training example 700 / 1031. Loss: 8523.9931640625\n",
      "Training example 800 / 1031. Loss: 8679.0107421875\n",
      "Training example 900 / 1031. Loss: 8712.2568359375\n",
      "Training example 1000 / 1031. Loss: 8643.0595703125\n",
      "------- EPOCH 11 --------\n",
      "Training example 100 / 1031. Loss: 11332.7822265625\n",
      "Training example 200 / 1031. Loss: 8681.5546875\n",
      "Training example 300 / 1031. Loss: 8607.8505859375\n",
      "Training example 400 / 1031. Loss: 8658.69140625\n",
      "Training example 500 / 1031. Loss: 8795.935546875\n",
      "Training example 600 / 1031. Loss: 8557.455078125\n",
      "Training example 700 / 1031. Loss: 8587.4765625\n",
      "Training example 800 / 1031. Loss: 8499.5703125\n",
      "Training example 900 / 1031. Loss: 8589.2841796875\n",
      "Training example 1000 / 1031. Loss: 8588.2900390625\n",
      "------- EPOCH 12 --------\n",
      "Training example 100 / 1031. Loss: 11168.1904296875\n",
      "Training example 200 / 1031. Loss: 8545.5390625\n",
      "Training example 300 / 1031. Loss: 8595.1025390625\n",
      "Training example 400 / 1031. Loss: 8596.5556640625\n",
      "Training example 500 / 1031. Loss: 8419.447265625\n",
      "Training example 600 / 1031. Loss: 8526.5693359375\n",
      "Training example 700 / 1031. Loss: 8513.46875\n",
      "Training example 800 / 1031. Loss: 8503.138671875\n",
      "Training example 900 / 1031. Loss: 8622.9912109375\n",
      "Training example 1000 / 1031. Loss: 8654.5556640625\n",
      "------- EPOCH 13 --------\n",
      "Training example 100 / 1031. Loss: 11129.658203125\n",
      "Training example 200 / 1031. Loss: 8632.384765625\n",
      "Training example 300 / 1031. Loss: 8515.37890625\n",
      "Training example 400 / 1031. Loss: 8619.677734375\n",
      "Training example 500 / 1031. Loss: 8393.2041015625\n",
      "Training example 600 / 1031. Loss: 8372.7548828125\n",
      "Training example 700 / 1031. Loss: 8565.3798828125\n",
      "Training example 800 / 1031. Loss: 8491.9453125\n",
      "Training example 900 / 1031. Loss: 8613.19921875\n",
      "Training example 1000 / 1031. Loss: 8530.826171875\n",
      "------- EPOCH 14 --------\n",
      "Training example 100 / 1031. Loss: 11077.9501953125\n",
      "Training example 200 / 1031. Loss: 8515.9326171875\n",
      "Training example 300 / 1031. Loss: 8448.0205078125\n",
      "Training example 400 / 1031. Loss: 8427.3916015625\n",
      "Training example 500 / 1031. Loss: 8518.796875\n",
      "Training example 600 / 1031. Loss: 8451.091796875\n",
      "Training example 700 / 1031. Loss: 8467.970703125\n",
      "Training example 800 / 1031. Loss: 8473.580078125\n",
      "Training example 900 / 1031. Loss: 8524.67578125\n",
      "Training example 1000 / 1031. Loss: 8442.0087890625\n",
      "------- EPOCH 15 --------\n",
      "Training example 100 / 1031. Loss: 10972.06640625\n",
      "Training example 200 / 1031. Loss: 8409.107421875\n",
      "Training example 300 / 1031. Loss: 8402.0263671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example 400 / 1031. Loss: 8472.234375\n",
      "Training example 500 / 1031. Loss: 8408.7021484375\n",
      "Training example 600 / 1031. Loss: 8401.4755859375\n",
      "Training example 700 / 1031. Loss: 8391.625\n",
      "Training example 800 / 1031. Loss: 8531.9501953125\n",
      "Training example 900 / 1031. Loss: 8502.255859375\n",
      "Training example 1000 / 1031. Loss: 8483.0361328125\n",
      "------- EPOCH 16 --------\n",
      "Training example 100 / 1031. Loss: 10969.6826171875\n",
      "Training example 200 / 1031. Loss: 8355.63671875\n",
      "Training example 300 / 1031. Loss: 8398.3671875\n",
      "Training example 400 / 1031. Loss: 8413.916015625\n",
      "Training example 500 / 1031. Loss: 8413.9521484375\n",
      "Training example 600 / 1031. Loss: 8429.25390625\n",
      "Training example 700 / 1031. Loss: 8469.0859375\n",
      "Training example 800 / 1031. Loss: 8436.5732421875\n",
      "Training example 900 / 1031. Loss: 8329.845703125\n",
      "Training example 1000 / 1031. Loss: 8384.6884765625\n",
      "------- EPOCH 17 --------\n",
      "Training example 100 / 1031. Loss: 11096.28515625\n",
      "Training example 200 / 1031. Loss: 8315.9072265625\n",
      "Training example 300 / 1031. Loss: 8421.0048828125\n",
      "Training example 400 / 1031. Loss: 8249.044921875\n",
      "Training example 500 / 1031. Loss: 8294.154296875\n",
      "Training example 600 / 1031. Loss: 8357.9560546875\n",
      "Training example 700 / 1031. Loss: 8376.9970703125\n",
      "Training example 800 / 1031. Loss: 8409.53125\n",
      "Training example 900 / 1031. Loss: 8362.3671875\n",
      "Training example 1000 / 1031. Loss: 8489.12109375\n",
      "------- EPOCH 18 --------\n",
      "Training example 100 / 1031. Loss: 10966.3046875\n",
      "Training example 200 / 1031. Loss: 8307.1181640625\n",
      "Training example 300 / 1031. Loss: 8321.1748046875\n",
      "Training example 400 / 1031. Loss: 8418.515625\n",
      "Training example 500 / 1031. Loss: 8365.505859375\n",
      "Training example 600 / 1031. Loss: 8339.1181640625\n",
      "Training example 700 / 1031. Loss: 8292.4365234375\n",
      "Training example 800 / 1031. Loss: 8368.0986328125\n",
      "Training example 900 / 1031. Loss: 8343.0341796875\n",
      "Training example 1000 / 1031. Loss: 8314.1435546875\n",
      "------- EPOCH 19 --------\n",
      "Training example 100 / 1031. Loss: 11064.875\n",
      "Training example 200 / 1031. Loss: 8377.9755859375\n",
      "Training example 300 / 1031. Loss: 8311.361328125\n",
      "Training example 400 / 1031. Loss: 8287.58984375\n",
      "Training example 500 / 1031. Loss: 8300.4423828125\n",
      "Training example 600 / 1031. Loss: 8223.5107421875\n",
      "Training example 700 / 1031. Loss: 8318.9296875\n",
      "Training example 800 / 1031. Loss: 8298.673828125\n",
      "Training example 900 / 1031. Loss: 8326.6875\n",
      "Training example 1000 / 1031. Loss: 8318.966796875\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 16 19:05:29 2019\n",
    "\n",
    "@author: karm2204\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "References:\n",
    "\"\"\"\n",
    "#%%\n",
    "\n",
    "# https://towardsdatascience.com/model-summary-in-pytorch-b5a1e4b64d25\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://discuss.pytorch.org/t/text-autoencoder-nan-loss-after-first-batch/22730\n",
    "# https://discuss.pytorch.org/t/understanding-output-padding-cnn-autoencoder-input-output-not-the-same/22743\n",
    "# https://github.com/rtqichen/beta-tcvae/blob/master/vae_quant.py\n",
    "# https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# https://www.groundai.com/project/isolating-sources-of-disentanglement-in-variational-autoencoders/\n",
    "# https://arogozhnikov.github.io/einops/pytorch-examples.html\n",
    "\n",
    "# Note   :    https://www.cs.toronto.edu/~lczhang/360/lec/w03/convnet.html\n",
    "#%%\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import cuda\n",
    "import torch.utils.dataF\n",
    "from torch import optim, autograd\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from GAN_q3 import Generator\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#%%    \n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5, .5, .5),\n",
    "                         (.5, .5, .5))\n",
    "])\n",
    "    \n",
    "def get_data_loader(dataset_location, batch_size):\n",
    "    trainvalid = torchvision.datasets.SVHN(\n",
    "        dataset_location, split='train',\n",
    "        download=True,\n",
    "        transform=image_transform\n",
    "    )\n",
    "\n",
    "    trainset_size = int(len(trainvalid) * 0.9)\n",
    "    trainset, validset = dataset.random_split(\n",
    "        trainvalid,\n",
    "        [trainset_size, len(trainvalid) - trainset_size]\n",
    "    )\n",
    "\n",
    "    train = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.SVHN(\n",
    "            dataset_location, split='test',\n",
    "            download=True,\n",
    "            transform=image_transform\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_data_loader_1(dataset_location, batch_size):\n",
    "    trainvalid = torchvision.datasets.SVHN(\n",
    "        dataset_location, split='train',\n",
    "        download=True,\n",
    "        transform=image_transform\n",
    "    )\n",
    "\n",
    "    trainset_size = int(len(trainvalid) * 0.9)\n",
    "    trainset, validset = dataset.random_split(\n",
    "        trainvalid,\n",
    "        [trainset_size, len(trainvalid) - trainset_size]\n",
    "    )\n",
    "\n",
    "    train = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.SVHN(\n",
    "            dataset_location, split='test',\n",
    "            download=True,\n",
    "            transform=image_transform_1\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "image_transform_1 = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#%%\n",
    "def imshow(img):\n",
    "    img = 0.5*(img + 1)\n",
    "    npimg = img.numpy()\n",
    "    # npimg = (255*npimg).astype(np.uint8) # to be a int in (0,...,255)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()                 \n",
    "                    \n",
    "#%% \n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape, *shape_):\n",
    "        super().__init__()\n",
    "        if isinstance(shape, list):\n",
    "            self.shape = shape\n",
    "        else:\n",
    "            self.shape = (shape,) + shape_      \n",
    "            \n",
    "def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "                    \n",
    "#%%                \n",
    "# https://discuss.pytorch.org/t/text-autoencoder-nan-loss-after-first-batch/22730\n",
    "                    \n",
    "#%% \n",
    "class convencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(convencoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size = 2, stride = 2),\n",
    "            nn.Conv2d(64, 256, kernel_size=6),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.final = nn.Linear(256, self.latent_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.final(x)\n",
    "        # Return mu and logvar\n",
    "        return x[..., self.latent_dim:], x[..., :self.latent_dim]         \n",
    "  \n",
    "#%% \n",
    "class convdecoder(Generator): # call generator from GAN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1, 1, 1)\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "#%%\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = convencoder()\n",
    "        self.decoder = convdecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "#        mu, logvar = convencoder[:, :self.latent_dim], convencoder[:, self.latent_dim:]\n",
    "        std = torch.exp(logvar / 2)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def ELBO(output, target, mu, logvar):\n",
    "    elbo = -torch.nn.functional.mse_loss(output, target, reduction='sum')\n",
    "    elbo += 0.5 * torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar))\n",
    "    return elbo / output.size(0)\n",
    "\n",
    "# https://www.groundai.com/project/isolating-sources-of-disentanglement-in-variational-autoencoders/\n",
    "#%%\n",
    "def visual_samples(vae, dimensions, device, svhn_loader):\n",
    "    z = torch.randn(64, dimensions, device = device)\n",
    "    generated = vae.decoder(z)\n",
    "    torchvision.utils.save_image(generated, 'images/vae/3.1vae-generated.png', normalize=False)\n",
    "    \n",
    "#%%\n",
    "def disentangled_representation(vae, dimensions, device, epsilon = 3):\n",
    "    z = torch.randn(dimensions, device = device)\n",
    "    z = z.repeat(dimensions+1, 1)\n",
    "    for i, sample in enumerate(z[1:]):\n",
    "        sample[i] += epsilon\n",
    "\n",
    "    generated = vae.decoder(z)\n",
    "    torchvision.utils.save_image(generated, 'images/vae/3_2positive_eps.png', normalize=False)\n",
    "    epsilon = -2*epsilon\n",
    "    for i, sample in enumerate(z[1:]):\n",
    "        sample[i] += epsilon\n",
    "\n",
    "    generated = vae.decoder(z)\n",
    "    torchvision.utils.save_image(generated, 'images/vae/3_2negative_eps.png', normalize=False)\n",
    "\n",
    "#%%\n",
    "    \n",
    "def interpolation(vae, dimensions, device):\n",
    "    # Interpolate in the latent space between z_0 and z_1\n",
    "    z_0 = torch.randn(1,dimensions, device=device)\n",
    "    z_1 = torch.randn(1,dimensions, device=device)\n",
    "    z_a = torch.zeros([11,dimensions], device=device)\n",
    "\n",
    "    for i in range(11):\n",
    "        a = i/10\n",
    "        z_a[i] = a*z_0 + (1-a)*z_1\n",
    "\n",
    "    generated = vae.decoder(z_a)\n",
    "    torchvision.utils.save_image(generated, 'images/vae/3_3latent.png', normalize=False)\n",
    "    \n",
    "    # Interpolate in the data space between x_0 and x_1\n",
    "    x_0 = vae.decoder(z_0)\n",
    "    x_1 = vae.decoder(z_1)\n",
    "    x_a = torch.zeros(11,x_0.size()[1],x_0.size()[2],x_0.size()[3], device = device)\n",
    "\n",
    "    for i in range(11):\n",
    "        a = i/10\n",
    "        x_a[i] = a*x_0 + (1-a)*x_1\n",
    "\n",
    "    torchvision.utils.save_image(x_a, 'images/vae/3_3data.png', normalize=False)   \n",
    "    \n",
    "    \n",
    "#%%\n",
    "def save_images(img_dir: str):\n",
    "    import os\n",
    "    vae = VAE()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae.load_state_dict(torch.load('VAE_q3_save.pth', map_location=device))\n",
    "    vae = vae.to(device)\n",
    "    vae.eval()\n",
    "    \n",
    "    for p in vae.parameters():\n",
    "        p.requires_grad = False\n",
    "        os.makedirs(f\"{img_dir}/img/\", exist_ok=True)\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        latents = torch.randn(100, 100, device=device)\n",
    "        images = vae.decoder(latents)\n",
    "        for j, image in enumerate(images):\n",
    "            filename = f\"images/vae/fid/img/{i * 100 + j:03d}.png\"\n",
    "            torchvision.utils.save_image(image, filename, normalize=True)\n",
    "            \n",
    "#%%        \n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Let's use {}\".format(device))\n",
    "    vae = VAE()\n",
    "    vae = vae.to(device)\n",
    "    running_loss = 0\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=3e-4)\n",
    "    train, valid, test = get_data_loader(\"svhn\", batch_size = 64)\n",
    "    try: \n",
    "        vae.load_state_dict(torch.load('VAE_q3_save.pth', map_location=device))\n",
    "        print('----Using saved model----')\n",
    "    except FileNotFoundError:\n",
    "        for epoch in range(20):\n",
    "            print(f\"------- EPOCH {epoch} --------\")\n",
    "            for i, (x, _) in enumerate(train):\n",
    "                vae.train()\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(device)\n",
    "                y, mu, logvar = vae(x)\n",
    "                loss = -ELBO(y, x, mu, logvar)\n",
    "                running_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if(i%10 == 0):\n",
    "                    visual_samples(vae, 100, device, test)\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"Training example {i + 1} / {len(train)}. Loss: {running_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        torch.save(vae.state_dict(), 'VAE_q3_save.pth')\n",
    "\n",
    "    dimensions = 100\n",
    "    \n",
    "    \n",
    "    visual_samples(vae, dimensions, device, test)\n",
    "    disentangled_representation(vae, dimensions, device, epsilon=10)\n",
    "    interpolation(vae, dimensions, device)\n",
    "    \n",
    "    img_dir = \"images/vae/fid\"\n",
    "    save_images(img_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
