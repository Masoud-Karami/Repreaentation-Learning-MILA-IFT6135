# -*- coding: utf-8 -*-
"""density_est.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16nxHd2DoPwblQ55Y6_u23boTsBn3_MuX
"""

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sat Mar 23 13:20:15 2019

@author: chin-weihuang
"""


from __future__ import print_function
import numpy as np
import torch 
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F
import samplers
import jsloss
from jsloss import Discriminator as Discriminator

# plot p0 and p1
plt.figure()

# empirical
xx = torch.randn(10000)
f = lambda x: torch.tanh(x*2+1) + x*0.75
d = lambda x: (1-torch.tanh(x*2+1)**2)*2+0.75
plt.hist(f(xx), 100, alpha=0.5, density=1)
plt.hist(xx, 100, alpha=0.5, density=1)
plt.xlim(-5,5)
# exact
xx = np.linspace(-5,5,1000)
N = lambda x: np.exp(-x**2/2.)/((2*np.pi)**0.5)
plt.plot(f(torch.from_numpy(xx)).numpy(), d(torch.from_numpy(xx)).numpy()**(-1)*N(xx))
plt.plot(xx, N(xx))


############### import the sampler ``samplers.distribution4'' 
############### train a discriminator on distribution4 and standard gaussian
############### estimate the density of distribution4

#######--- INSERT YOUR CODE BELOW ---#######
epoch_count =25000
unk_disc = Discriminator(input_size = 1).cuda()
unk_loss = nn.BCELoss(reduction = 'mean')
unk_optim = optim.SGD(unk_disc.parameters(), lr=1e-3)

for i in range(epoch_count):
    unk_disc.zero_grad()
    
    #get data for both distributions from samplers
    real_dist = iter(samplers.distribution4(512))
    real_samples = next(real_dist)
    real_tensor_samples = torch.tensor(real_samples, requires_grad = True).float().cuda()

    fake_dist = iter(samplers.distribution3(512)) 
    fake_samples = next(fake_dist)
    fake_tensor_samples = torch.tensor(fake_samples, requires_grad = True).float().cuda()

    real_targets = torch.ones([len(real_tensor_samples),1]).cuda()
    fake_targets = torch.zeros([len(fake_tensor_samples),1]).cuda()



    real_output = unk_disc(real_tensor_samples)
    fake_output = unk_disc(fake_tensor_samples)

    loss_real_output = unk_loss(real_output, real_targets)
    loss_fake_output = unk_loss(fake_output, fake_targets)

    total_output = loss_real_output + loss_fake_output

    total_output.backward()
    unk_optim.step()


############### plotting things
############### (1) plot the output of your trained discriminator 
############### (2) plot the estimated density contrasted with the true density

 
r = unk_disc(torch.from_numpy(xx).float().cuda().unsqueeze(1)).cpu().detach().numpy()  # evaluate xx using your discriminator; replace xx with the output
plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
plt.plot(xx,r)
plt.title(r'$D(x)$')

estimate = np.multiply((r/(1-r)).reshape(-1),(np.exp(-xx**2/2.)/((2*np.pi)**0.5))) # estimate the density of distribution4 (on xx) using the discriminator; 
                                # replace "np.ones_like(xx)*0." with your estimate
plt.subplot(1,2,2)
plt.plot(xx,estimate)
plt.plot(f(torch.from_numpy(xx)).numpy(), d(torch.from_numpy(xx)).numpy()**(-1)*N(xx))
plt.legend(['Estimated','True'])
plt.title('Estimated vs True')

