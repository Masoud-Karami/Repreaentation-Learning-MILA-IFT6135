{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE_Q2.ipynb","version":"0.3.2","provenance":[{"file_id":"1DeGa5KAHUgYu78vExHzYfpNHU5zcp3AY","timestamp":1555981877620}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"a-HwxN4Iyg_j","colab_type":"code","outputId":"90fdfc8b-5460-4a4d-f318-8dd534f7f746","executionInfo":{"status":"ok","timestamp":1555983178988,"user_tz":240,"elapsed":17781,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Apr 16 09:52:14 2019\n","\n","@author: karm2204\n","\"\"\"\n","\n","\n","\n","#####         https://chrisorm.github.io/VAE-pyt.html\n","#             Variational Autoencoder in Pytorch\n","#             https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py\n","#             https://github.com/bobchennan/VAE_NBP/blob/master/vae_dp.py\n","#             https://github.com/pytorch/examples/blob/master/vae/main.py\n","\n","from __future__ import print_function\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms, datasets\n","from torchvision.utils import save_image\n","from torch.autograd import Variable\n","from sklearn.mixture import BayesianGaussianMixture\n","import torch.optim as optim\n","import torch.utils.data\n","import argparse\n","from torchvision.datasets import utils\n","import torch.utils.data as data_utils\n","import torch\n","import os\n","import numpy as np\n","from torch import nn\n","from torch.nn.modules import upsampling\n","from torch.functional import F\n","from torch.optim import Adam\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.set_printoptions(precision=4)\n","\n","\n","# Hyper-parameters\n","batch_size = 128\n","learning_rate = 3e-4\n","image_size = 784\n","h_dim = 400\n","z_dim = 100\n","num_epochs = 20\n","\n","\n","#%%\n","\n","def get_data_loader(dataset_location, batch_size):\n","    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n","    # start processing\n","    def lines_to_np_array(lines):\n","        return np.array([[int(i) for i in line.split()] for line in lines])\n","    splitdata = []\n","    for splitname in [\"train\", \"valid\", \"test\"]:\n","        filename = \"binarized_mnist_%s.amat\" % splitname\n","        filepath = os.path.join(dataset_location, filename)\n","        utils.download_url(URL + filename, dataset_location)\n","        with open(filepath) as f:\n","            lines = f.readlines()\n","        x = lines_to_np_array(lines).astype('float32')\n","        x = x.reshape(x.shape[0], 1, 28, 28)\n","        # pytorch data loader\n","        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n","        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n","        splitdata.append(dataset_loader)\n","    return splitdata\n","\n","train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", batch_size)\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","for x in train_loader:\n","    plt.imshow(x[0, 0])\n","    break\n","    \n","torch.cuda.memory_allocated(device)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["17029120"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC3VJREFUeJzt3V2oZfV5x/Hvr3Yc6SQFp2mHqZGa\nBimI0Ek5TAqRkmKTGglobiRehClIJhcRGshFxV7USylNghclMGmGjCU1KSSiF9LEDgUJFPEoxpfY\nRiMTMtNxxjABTaHjaJ5enDXhqOfNs1/WPj7fDxz22mutfdYzi/mdtfZ61t7/VBWS+vmNsQuQNA7D\nLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pqd+c58Yuze66jD3z3KTUyv/xv7xW57OVdScKf5Ib\ngHuAS4B/qqq7N1r/Mvbw4Vw/ySYlbeDROr7ldbd92p/kEuAfgU8A1wC3Jrlmu79P0nxN8p7/IPBC\nVb1YVa8B3wJumk5ZkmZtkvBfAfxs1fOTw7w3SXI4yXKS5Qucn2BzkqZp5lf7q+pIVS1V1dIuds96\nc5K2aJLwnwKuXPX8/cM8STvAJOF/DLg6yQeSXAp8GnhwOmVJmrVtt/qq6vUktwPfY6XVd7Sqnp1a\nZZJmaqI+f1U9BDw0pVokzZG390pNGX6pKcMvNWX4paYMv9SU4ZeaMvxSU4ZfasrwS00Zfqkpwy81\nZfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMv\nNWX4paYMv9TURKP0JjkBvAq8AbxeVUvTKEqL43v/8+TYJazrL3//wNgl7GgThX/w51X18yn8Hklz\n5Gm/1NSk4S/g+0keT3J4GgVJmo9JT/uvq6pTSX4PeDjJf1XVI6tXGP4oHAa4jN+acHOSpmWiI39V\nnRoezwL3AwfXWOdIVS1V1dIudk+yOUlTtO3wJ9mT5L0Xp4GPA89MqzBJszXJaf8+4P4kF3/Pv1TV\nv02lKkkzt+3wV9WLwB9PsZa2Nuulb9bPnmUvfsxta7Zs9UlNGX6pKcMvNWX4paYMv9SU4Zeamsan\n+jRjY7byZv36jWz27560RdqdR36pKcMvNWX4paYMv9SU4ZeaMvxSU4Zfaso+/xaN+dFV+9WaBY/8\nUlOGX2rK8EtNGX6pKcMvNWX4paYMv9SUff6Bffz582u/x+WRX2rK8EtNGX6pKcMvNWX4paYMv9SU\n4Zea2rTPn+Qo8EngbFVdO8zbC3wbuAo4AdxSVb+YXZmzN8uhqLv28WfN/TqZrRz5vwHc8JZ5dwDH\nq+pq4PjwXNIOsmn4q+oR4NxbZt8EHBumjwE3T7kuSTO23ff8+6rq9DD9ErBvSvVImpOJL/hVVQG1\n3vIkh5MsJ1m+wPlJNydpSrYb/jNJ9gMMj2fXW7GqjlTVUlUt7WL3Njcnadq2G/4HgUPD9CHggemU\nI2leNg1/kvuA/wT+KMnJJLcBdwMfS/I88BfDc0k7yKZ9/qq6dZ1F10+5loVmT3l7vD9icXmHn9SU\n4ZeaMvxSU4ZfasrwS00Zfqkpv7pbGxrz67U327atwMl45JeaMvxSU4ZfasrwS00Zfqkpwy81Zfil\npuzzN7eTh8n2PoDJeOSXmjL8UlOGX2rK8EtNGX6pKcMvNWX4pabs8+8AO7kXv5FZDos+6es73CPg\nkV9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmtq0z5/kKPBJ4GxVXTvMuwv4LPDysNqdVfXQrIp8txuz\nj7/I/exJa3u33h8xLVs58n8DuGGN+V+pqgPDj8GXdphNw19VjwDn5lCLpDma5D3/7UmeSnI0yeVT\nq0jSXGw3/F8FPggcAE4DX1pvxSSHkywnWb7A+W1uTtK0bSv8VXWmqt6oql8BXwMObrDukapaqqql\nXezebp2Spmxb4U+yf9XTTwHPTKccSfOylVbffcBHgfclOQn8HfDRJAeAAk4An5thjZJmIFU1t439\ndvbWh3P93Lb3bjHLfvUi9/nHtFPHBHi0jvNKnctW1vUOP6kpwy81Zfilpgy/1JThl5oy/FJTfnX3\nu8Citp202DzyS00Zfqkpwy81Zfilpgy/1JThl5oy/FJT9vl3APv40+fXenvkl9oy/FJThl9qyvBL\nTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMvNWX4paY2/Tx/kiuBe4F9QAFHquqe\nJHuBbwNXASeAW6rqF7MrVXozhy6fzFaO/K8DX6yqa4A/BT6f5BrgDuB4VV0NHB+eS9ohNg1/VZ2u\nqieG6VeB54ArgJuAY8Nqx4CbZ1WkpOl7R+/5k1wFfAh4FNhXVaeHRS+x8rZA0g6x5fAneQ/wHeAL\nVfXK6mVVVaxcD1jrdYeTLCdZvsD5iYqVND1bCn+SXawE/5tV9d1h9pkk+4fl+4Gza722qo5U1VJV\nLe1i9zRqljQFm4Y/SYCvA89V1ZdXLXoQODRMHwIemH55kmZlK1/d/RHgM8DTSS72Vu4E7gb+Nclt\nwE+BW2ZT4uLbrOXUoW00C7byZmvT8FfVD4Css/j66ZYjaV68w09qyvBLTRl+qSnDLzVl+KWmDL/U\nlEN0z0Hn+wDs1S8uj/xSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JR9/gUwy164tB6P/FJThl9qyvBL\nTRl+qSnDLzVl+KWmDL/UlH3+Kdjsc+Wd+/h+5n5xeeSXmjL8UlOGX2rK8EtNGX6pKcMvNWX4paY2\n7fMnuRK4F9gHFHCkqu5JchfwWeDlYdU7q+qhWRW6k9nr1iLayk0+rwNfrKonkrwXeDzJw8Oyr1TV\nP8yuPEmzsmn4q+o0cHqYfjXJc8AVsy5M0my9o/f8Sa4CPgQ8Osy6PclTSY4muXyd1xxOspxk+QLn\nJypW0vRsOfxJ3gN8B/hCVb0CfBX4IHCAlTODL631uqo6UlVLVbW0i91TKFnSNGwp/El2sRL8b1bV\ndwGq6kxVvVFVvwK+BhycXZmSpm3T8CcJ8HXguar68qr5+1et9ingmemXJ2lWtnK1/yPAZ4Cnk1z8\nbOqdwK1JDrDS/jsBfG4mFUqaia1c7f8BkDUW2dOXdjDv8JOaMvxSU4ZfasrwS00Zfqkpwy81Zfil\npgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzWVqprfxpKXgZ+umvU+4OdzK+CdWdTaFrUusLbtmmZt\nf1BVv7uVFeca/rdtPFmuqqXRCtjAota2qHWBtW3XWLV52i81ZfilpsYO/5GRt7+RRa1tUesCa9uu\nUWob9T2/pPGMfeSXNJJRwp/khiT/neSFJHeMUcN6kpxI8nSSJ5Msj1zL0SRnkzyzat7eJA8neX54\nXHOYtJFquyvJqWHfPZnkxpFquzLJfyT5UZJnk/z1MH/UfbdBXaPst7mf9ie5BPgx8DHgJPAYcGtV\n/WiuhawjyQlgqapG7wkn+TPgl8C9VXXtMO/vgXNVdffwh/PyqvqbBantLuCXY4/cPAwos3/1yNLA\nzcBfMeK+26CuWxhhv41x5D8IvFBVL1bVa8C3gJtGqGPhVdUjwLm3zL4JODZMH2PlP8/crVPbQqiq\n01X1xDD9KnBxZOlR990GdY1ijPBfAfxs1fOTLNaQ3wV8P8njSQ6PXcwa9g3DpgO8BOwbs5g1bDpy\n8zy9ZWTphdl32xnxetq84Pd211XVnwCfAD4/nN4upFp5z7ZI7Zotjdw8L2uMLP1rY+677Y54PW1j\nhP8UcOWq5+8f5i2Eqjo1PJ4F7mfxRh8+c3GQ1OHx7Mj1/Noijdy81sjSLMC+W6QRr8cI/2PA1Uk+\nkORS4NPAgyPU8TZJ9gwXYkiyB/g4izf68IPAoWH6EPDAiLW8yaKM3LzeyNKMvO8WbsTrqpr7D3Aj\nK1f8fwL87Rg1rFPXHwI/HH6eHbs24D5WTgMvsHJt5Dbgd4DjwPPAvwN7F6i2fwaeBp5iJWj7R6rt\nOlZO6Z8Cnhx+bhx7321Q1yj7zTv8pKa84Cc1Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qan/Bynz\nyoI6SQpEAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"sLyvHhhGKshB","colab_type":"code","outputId":"9efb5008-ad53-4402-d552-43caefce5e1b","executionInfo":{"status":"ok","timestamp":1555982003636,"user_tz":240,"elapsed":17227,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["next(enumerate(train_loader))[1].size()\n","torch.cuda.memory_allocated(device)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"GUfU7LW-1Jut","colab_type":"code","outputId":"eaca0020-7458-4a1e-a55d-c05788e6845c","executionInfo":{"status":"ok","timestamp":1555982003637,"user_tz":240,"elapsed":16706,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#%%\n","class VAE(nn.Module):\n","    def __init__(self, image_size=784, h_dim=400, z_dim=100):\n","        super(VAE, self).__init__()\n","        \n","#         self.encoder = nn.Sequential(nn.Linear(image_size, h_dim),\\\n","#                                 nn.ReLU()\\\n","#                                )\n","        self.encoder= nn.Sequential(nn.Conv2d(1,32,3),\\\n","                                    nn.ELU(),\\\n","                                    nn.AvgPool2d(2,2),\\\n","                                    nn.Conv2d(32,64,3),\\\n","                                    nn.ELU(),\\\n","                                    nn.AvgPool2d(2,2),\\\n","                                    nn.Conv2d(64,256,5),\\\n","                                    nn.ELU(),\\\n","                                    )\n","#         self.fc_1 = nn.Linear(image_size, h_dim)\n","        self.fc_1 = nn.Linear(256, z_dim)\n","        self.fc_2 = nn.Linear(256, z_dim)\n","\n","        self.fc_3 = nn.Linear(z_dim, 256)\n","#         self.fc_5 = nn.Linear(h_dim, image_size)\n","        self.decoder= nn.Sequential(nn.ELU(),\\\n","                                    nn.Conv2d(256,64,5, padding = 4),\\\n","                                    nn.ELU(),\\\n","                                    nn.UpsamplingBilinear2d(scale_factor=2),\\\n","                                    nn.Conv2d(64, 32, 3, padding=2),\\\n","                                    nn.ELU(),\\\n","                                    nn.UpsamplingBilinear2d(scale_factor=2),\\\n","                                    nn.Conv2d(32, 16, 3, padding=2),\\\n","                                    nn.ELU(),\\\n","                                    nn.Conv2d(16, 1, 3, padding=2),\\\n","                                    nn.Sigmoid()\n","                                    )\n","        \n","    \"\"\" Encode a batch of samples, and return posterior parameters for each point.\"\"\"    \n","    def encode(self, x):\n","#         h_1 = F.relu(self.fc_1(x))\n","      h_1 = self.encoder(x)\n","      return self.fc_1(h_1.view(-1,256)), self.fc_2(h_1.view(-1,256))\n","\n","\n","    \"\"\" Reparameterisation trick to sample z values. \n","        This is stochastic during training,  and returns the mode during evaluation.\n","        For each training sample (we get 128 batched at a time)\n","        - take the current learned mu, stddev for each of the z_dim \n","        (in the pytorch VAE example, this is 20, z_dim = 20)\n","          dimensions and draw a random sample from that distribution\n","        - the whole network is trained so that these randomly drawn\n","          samples decode to output that looks like the input\n","        - which will mean that the std, mu will be learned\n","          *distributions* that correctly encode the inputs\n","        - due to the additional KLD term (see loss_function() below)\n","          the distribution will tend to unit Gaussians\n","        Parameters\n","        ----------\n","        mu : [128, z_dim] mean matrix\n","        logvar : [128, z_dim] variance matrix\n","        Returns\n","        -------\n","        During training random sample from the learned ZDIMS-dimensional\n","        normal distribution; during inference its mean.\n","        \"\"\"\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(logvar/2)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","\n","    \"\"\" Decode a batch of latent variables\"\"\"\n","    def decode(self, z):\n","    #        h_3 = F.relu(self.fc_4(z))\n","    #        return F.sigmoid(self.fc_5(h_3))\n","        h_2 = self.fc_3(z).view(-1,256,1,1)\n","        h_3 = self.decoder(h_2)\n","        return h_3\n","\n","\n","    \"\"\" Takes a batch of samples, encodes them, and then decodes them again to compare.\"\"\"\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","torch.cuda.memory_allocated(device)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"ywvLbrAtzHaM","colab_type":"code","outputId":"dafffa9c-4e86-40c3-82da-b162f38f74fc","executionInfo":{"status":"ok","timestamp":1555982350464,"user_tz":240,"elapsed":198929,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":1601}},"cell_type":"code","source":["#%%\n","model = VAE().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\"\"\" ELBO assuming entries of x are binary variables, with closed form KLD.\"\"\"\n","def loss_function(x_reconst, x, mu, logvar):\n","    bce = F.binary_cross_entropy(x_reconst, x.view(-1, 784), reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    # Normalise by same number of elements as in reconstruction\n","    # KLD /= x.view(-1, image_size).data.shape[0] * image_size\n","    return bce + KLD\n","#%%\n","\n","# ----------\n","#  Train\n","# ----------\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    log_interval = 100\n","    for batch_idx, data in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)\n","        loss = loss_function(recon_batch, data, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset), 100. *\n","                batch_idx / len(train_loader), -loss.item() / len(data)\n","            ))\n","\n","    return -train_loss / len(train_loader.dataset)\n","#%%\n","\n","# ----------\n","#  Test\n","# ----------\n","def test(epoch, data_loader):\n","    model.eval()\n","    valid_loss = 0\n","    # ind = np.arange(x.shape[0])\n","    with torch.no_grad():\n","        for i, data in enumerate(data_loader):\n","            data = data.to(device)\n","#           test_data = torch.from_numpy(test_data[np.random.choice(ind, size=batch_size)])\n","#           test_data = Variable(test_data, requires_grad=False)\n","            reconst_batch, mu, logvar = model(data)\n","            valid_loss += loss_function(reconst_batch, data, mu, logvar).item()\n","            if i == 0:\n","                n = min(data.size(0), 28)\n","                comparison = torch.cat([data[:n],\n","                                        reconst_batch.view(128, 1, 28, 28)[:n]])\n","#                 save_image(comparison.cpu(),\n","#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n","#            print(data.view(batch_size, 2,2)[:n])\n","#            print(reconst_batch.view(batch_size, 2,2)[:n])\n","            \n","            \n","    valid_loss /= len(data_loader.dataset)\n","    return -valid_loss\n","  \n","#%%    \n","if __name__ == \"__main__\":\n","    for epoch in range(1, 21):\n","        train(epoch)\n","        test(epoch, valid_loader)\n","        # 64 sets of random z_dim-float vectors, i.e. 64 locations / MNIST\n","        # digits in latent space\n","        with torch.no_grad():\n","            sample = torch.randn(128, z_dim).to(device)\n","            sample = model.decode(sample).cpu()\n","#             save_image(sample.view(64, 1, 28, 28),\n","#                        'results/sample_' + str(epoch) + '.png')\n","    torch.save(model.state_dict(), 'vae.pth')            \n","            "],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n","  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/50000 (0%)]\t Loss: -532.815552\n","Train Epoch: 1 [12800/50000 (26%)]\t Loss: -208.701523\n","Train Epoch: 1 [25600/50000 (51%)]\t Loss: -181.784576\n","Train Epoch: 1 [38400/50000 (77%)]\t Loss: -161.151993\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([80, 784])) that is different to the input size (torch.Size([80, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n","  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([16, 784])) that is different to the input size (torch.Size([16, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n","  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 2 [0/50000 (0%)]\t Loss: -154.716049\n","Train Epoch: 2 [12800/50000 (26%)]\t Loss: -153.167694\n","Train Epoch: 2 [25600/50000 (51%)]\t Loss: -145.251129\n","Train Epoch: 2 [38400/50000 (77%)]\t Loss: -135.755966\n","Train Epoch: 3 [0/50000 (0%)]\t Loss: -128.121307\n","Train Epoch: 3 [12800/50000 (26%)]\t Loss: -125.251556\n","Train Epoch: 3 [25600/50000 (51%)]\t Loss: -127.296944\n","Train Epoch: 3 [38400/50000 (77%)]\t Loss: -123.516449\n","Train Epoch: 4 [0/50000 (0%)]\t Loss: -114.553139\n","Train Epoch: 4 [12800/50000 (26%)]\t Loss: -121.403183\n","Train Epoch: 4 [25600/50000 (51%)]\t Loss: -116.713455\n","Train Epoch: 4 [38400/50000 (77%)]\t Loss: -111.704681\n","Train Epoch: 5 [0/50000 (0%)]\t Loss: -109.960449\n","Train Epoch: 5 [12800/50000 (26%)]\t Loss: -106.409088\n","Train Epoch: 5 [25600/50000 (51%)]\t Loss: -104.866890\n","Train Epoch: 5 [38400/50000 (77%)]\t Loss: -109.222549\n","Train Epoch: 6 [0/50000 (0%)]\t Loss: -109.645836\n","Train Epoch: 6 [12800/50000 (26%)]\t Loss: -107.572144\n","Train Epoch: 6 [25600/50000 (51%)]\t Loss: -104.694664\n","Train Epoch: 6 [38400/50000 (77%)]\t Loss: -104.715714\n","Train Epoch: 7 [0/50000 (0%)]\t Loss: -101.899323\n","Train Epoch: 7 [12800/50000 (26%)]\t Loss: -99.873505\n","Train Epoch: 7 [25600/50000 (51%)]\t Loss: -104.525871\n","Train Epoch: 7 [38400/50000 (77%)]\t Loss: -107.349182\n","Train Epoch: 8 [0/50000 (0%)]\t Loss: -102.891983\n","Train Epoch: 8 [12800/50000 (26%)]\t Loss: -102.644310\n","Train Epoch: 8 [25600/50000 (51%)]\t Loss: -107.889404\n","Train Epoch: 8 [38400/50000 (77%)]\t Loss: -100.763680\n","Train Epoch: 9 [0/50000 (0%)]\t Loss: -101.520874\n","Train Epoch: 9 [12800/50000 (26%)]\t Loss: -100.799919\n","Train Epoch: 9 [25600/50000 (51%)]\t Loss: -99.405273\n","Train Epoch: 9 [38400/50000 (77%)]\t Loss: -100.850281\n","Train Epoch: 10 [0/50000 (0%)]\t Loss: -99.599289\n","Train Epoch: 10 [12800/50000 (26%)]\t Loss: -97.731552\n","Train Epoch: 10 [25600/50000 (51%)]\t Loss: -99.122795\n","Train Epoch: 10 [38400/50000 (77%)]\t Loss: -101.746216\n","Train Epoch: 11 [0/50000 (0%)]\t Loss: -98.791824\n","Train Epoch: 11 [12800/50000 (26%)]\t Loss: -99.920769\n","Train Epoch: 11 [25600/50000 (51%)]\t Loss: -98.830124\n","Train Epoch: 11 [38400/50000 (77%)]\t Loss: -100.671844\n","Train Epoch: 12 [0/50000 (0%)]\t Loss: -97.955078\n","Train Epoch: 12 [12800/50000 (26%)]\t Loss: -104.448730\n","Train Epoch: 12 [25600/50000 (51%)]\t Loss: -96.532394\n","Train Epoch: 12 [38400/50000 (77%)]\t Loss: -96.856384\n","Train Epoch: 13 [0/50000 (0%)]\t Loss: -104.179962\n","Train Epoch: 13 [12800/50000 (26%)]\t Loss: -94.490234\n","Train Epoch: 13 [25600/50000 (51%)]\t Loss: -99.918610\n","Train Epoch: 13 [38400/50000 (77%)]\t Loss: -95.138138\n","Train Epoch: 14 [0/50000 (0%)]\t Loss: -99.457176\n","Train Epoch: 14 [12800/50000 (26%)]\t Loss: -102.657410\n","Train Epoch: 14 [25600/50000 (51%)]\t Loss: -98.182114\n","Train Epoch: 14 [38400/50000 (77%)]\t Loss: -94.347321\n","Train Epoch: 15 [0/50000 (0%)]\t Loss: -99.578720\n","Train Epoch: 15 [12800/50000 (26%)]\t Loss: -99.600632\n","Train Epoch: 15 [25600/50000 (51%)]\t Loss: -95.293732\n","Train Epoch: 15 [38400/50000 (77%)]\t Loss: -92.948036\n","Train Epoch: 16 [0/50000 (0%)]\t Loss: -98.336578\n","Train Epoch: 16 [12800/50000 (26%)]\t Loss: -95.199387\n","Train Epoch: 16 [25600/50000 (51%)]\t Loss: -95.970673\n","Train Epoch: 16 [38400/50000 (77%)]\t Loss: -96.787918\n","Train Epoch: 17 [0/50000 (0%)]\t Loss: -95.366653\n","Train Epoch: 17 [12800/50000 (26%)]\t Loss: -92.340202\n","Train Epoch: 17 [25600/50000 (51%)]\t Loss: -97.978882\n","Train Epoch: 17 [38400/50000 (77%)]\t Loss: -98.981621\n","Train Epoch: 18 [0/50000 (0%)]\t Loss: -97.594734\n","Train Epoch: 18 [12800/50000 (26%)]\t Loss: -95.040436\n","Train Epoch: 18 [25600/50000 (51%)]\t Loss: -98.188240\n","Train Epoch: 18 [38400/50000 (77%)]\t Loss: -97.055481\n","Train Epoch: 19 [0/50000 (0%)]\t Loss: -95.077690\n","Train Epoch: 19 [12800/50000 (26%)]\t Loss: -95.318748\n","Train Epoch: 19 [25600/50000 (51%)]\t Loss: -93.041145\n","Train Epoch: 19 [38400/50000 (77%)]\t Loss: -92.431023\n","Train Epoch: 20 [0/50000 (0%)]\t Loss: -97.253746\n","Train Epoch: 20 [12800/50000 (26%)]\t Loss: -97.855759\n","Train Epoch: 20 [25600/50000 (51%)]\t Loss: -99.777039\n","Train Epoch: 20 [38400/50000 (77%)]\t Loss: -94.591599\n"],"name":"stdout"}]},{"metadata":{"id":"bDQ3dpFMI9vT","colab_type":"text"},"cell_type":"markdown","source":["Question 2.1\n","\n","The average per-instance ELBO at the bottom is the average per-instance ELBO on the validaton set."]},{"metadata":{"id":"k5jgsPekJzRy","colab_type":"code","colab":{}},"cell_type":"code","source":["train_elbo = test(20, train_loader)\n","valid_elbo = test(20, valid_loader)\n","test_elbo  = test(20, test_loader)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8DXSCvRzy0QJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"61d186d5-80e3-4e44-c9fd-8d55152bb0bb","executionInfo":{"status":"ok","timestamp":1555982666340,"user_tz":240,"elapsed":411,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}}},"cell_type":"code","source":["print(\"ELBOs of model:\")\n","print(train_elbo,' for the training set')\n","print(valid_elbo,' for the validation set')\n","print(test_elbo, ' for the test set')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["ELBOs of model:\n","-95.35714745117187  for the training set\n","-95.90588443603515  for the validation set\n","-95.14183071289062  for the test set\n"],"name":"stdout"}]},{"metadata":{"id":"ilFxgXH4-yTV","colab_type":"code","colab":{}},"cell_type":"code","source":["#Evaluating log-likelihood with Variational Autoencoders 2.1\n","\n","def importance_sampling_minibatch(model, images, samples):\n","  K = 200\n","  hidden_size = 100\n","  minibatch_size = 128\n","  image_dim = 28\n","  image_size = 784\n","  images.to(device)\n","  samples.to(device)\n","  samples = samples.permute(1,0,2)\n","  \n","  with torch.no_grad():\n","    mean_val = torch.mean(samples, dim = 0)\n","    sigma_val = torch.std(samples, dim = 0)\n","\n","    #two distributions, one standard and one displaced \n","    standard = torch.distributions.normal.Normal(torch.zeros(min(minibatch_size,mean_val.size(0)),hidden_size).cuda(),\\\n","                                                 torch.ones(min(minibatch_size,mean_val.size(0)),hidden_size).cuda())\n","    displaced = torch.distributions.normal.Normal(mean_val, sigma_val)\n","\n","#     #sample z x 200 from displaced distribution(mean and s.d. from encoder run on data)\n","#     samples = [displaced.sample(sample_shape=torch.Size()) for i in range(K)]\n","\n","    #p and q are (200, 128,100)\n","    #density values for each samples, for each minibatch, for each uncorrelated z_i\n","    log_p_samples = torch.stack([standard.log_prob(samples[i].data) for i in range(len(samples))])\n","    log_q_samples = torch.stack([displaced.log_prob(samples[i].data) for i in range(len(samples))])\n","\n","    #torch.Size([200, 128, 784]) reconstructions of each sample/minibatch/image\n","    reconstructions = torch.stack([model.decode(samples[i].data).view(-1,image_size) for i in range(K)])\n","\n","    #p of x given z computed with Binary cross entropy \n","    log_p_x_given_z = ((images.view(-1,image_size) * torch.log(torch.clamp(reconstructions.data, 1e-9, 1))) + \\\n","                       ((1.0-images.view(-1,image_size))*torch.log(torch.clamp(1.0-reconstructions.data, 1e-9, 1))))\n","     \n","    #max prob is 1, so max log prob is log(200) for logsumexp trick: \n","    return (-np.log(K) + torch.logsumexp(torch.sum(log_p_x_given_z, dim=-1) + torch.sum(log_p_samples, dim=-1) - \\\n","                                         torch.sum(log_q_samples, dim=-1), 0))\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MgpNleoGry9R","colab_type":"code","outputId":"cf069602-6bae-4c14-8451-94e1e0ad8153","executionInfo":{"status":"ok","timestamp":1555980543920,"user_tz":240,"elapsed":1638,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":638}},"cell_type":"code","source":["#Generating own samples for proof that importance_sampling_minibatch works properly\n","temp_images = next(enumerate(valid_loader))[1].view(-1,784).cuda()\n","temp_mu, temp_sigma = model.encode(temp_images.view(-1,1,28,28))\n","temp_sigma = torch.exp(temp_sigma.data/2)\n","\n","standard = torch.distributions.normal.Normal(torch.zeros(128,100).cuda(), torch.ones(128,100).cuda())\n","displaced = torch.distributions.normal.Normal(temp_mu, temp_sigma)\n","samples = [displaced.sample(sample_shape=torch.Size()) for i in range(200)]\n","samples = torch.stack(samples).permute(1,0,2)\n","\n","importance_sampling_minibatch(model, temp_images, samples)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["tensor([ -95.71740723,  -63.68551254, -111.92938995,  -72.35781097,\n","         -84.92098999,  -97.94902039, -106.89511108,  -72.77054596,\n","         -46.66260529,  -63.53670883,  -97.72489166,  -96.01272583,\n","        -108.42823029,  -94.70073700,  -76.68729401,  -83.60478210,\n","         -53.56590271,  -73.82296753, -109.99315643,  -89.52267456,\n","         -75.71301270,  -85.17124939,  -98.34494781,  -92.26325989,\n","        -108.07697296,  -54.44738007,  -93.37905121,  -48.34919739,\n","        -116.03975677,  -88.53775024,  -42.16527557, -116.36687469,\n","         -72.04990387, -110.46295929,  -55.04835129,  -94.51477814,\n","        -110.25427246,  -95.42953491,  -56.89253998, -107.49494171,\n","         -95.87160492, -109.50654602, -104.27269745, -100.05001068,\n","         -84.35396576,  -99.96205902, -101.85662842, -122.29142761,\n","        -113.62080383,  -86.54142761,  -76.62197876,  -43.35823059,\n","        -110.48961639,  -59.68756866,  -67.32563782,  -87.19290161,\n","         -40.65615082, -107.88906097,  -76.66815186,  -73.15802002,\n","         -95.72063446,  -68.62629700, -115.53807068, -105.90459442,\n","         -67.23842621,  -79.66036224,  -87.70726013,  -85.82482147,\n","         -93.42596436, -102.41816711,  -80.21990204, -110.10641479,\n","         -48.92530823,  -72.46334076,  -45.80054092, -116.24835968,\n","         -82.83421326,  -46.31247711,  -85.46435547,  -72.10692596,\n","        -100.77931976, -114.41811371, -118.38143921,  -87.13324738,\n","        -100.28268433,  -90.02969360,  -98.87768555,  -85.66927338,\n","         -78.55163574,  -85.78139496, -113.20990753,  -79.34964752,\n","         -88.88858795, -112.38219452,  -99.20133209,  -93.73424530,\n","         -81.01017761,  -75.11617279,  -62.89939499, -112.74553680,\n","        -127.29337311, -101.55782318,  -78.86754608,  -88.99289703,\n","         -95.55976105, -123.38130951,  -91.43922424,  -96.34022522,\n","         -84.56596375,  -87.48748016,  -74.44283295,  -71.89260864,\n","         -96.76017761,  -93.50818634,  -74.83956146,  -96.54107666,\n","         -67.40249634,  -63.40478897,  -79.67208862, -103.47483063,\n","         -85.37796021,  -90.56900024,  -94.30204010,  -82.01316071,\n","        -120.22059631, -106.66234589, -112.60184479,  -69.67362976],\n","       device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"AMT_isebFmLC","colab_type":"text"},"cell_type":"markdown","source":["For more practical purposes, the following importance sampling function was used instead since it generates its own samples $z_{ik}$."]},{"metadata":{"id":"1SNlHGb6Fk82","colab_type":"code","colab":{}},"cell_type":"code","source":["def importance_sampling(trained_model, image_loader, minibatch_size, image_size, hidden_size, K):\n","  \n","  log_px = []\n","  with torch.no_grad():\n","\n","    #forward pass through model with image set\n","    for i, data in enumerate(image_loader):\n","      data = data.to(device)\n","      mean_val, log_var_val = model.encode(data)\n","      images = data\n","    #   mean_val_test = mean_val[0].data\n","    #   log_var_val_test = log_var_val[0].data\n","      sigma_val = torch.exp(log_var_val.data/2)\n","\n","      #two distributions, one standard and one displaced \n","      standard = torch.distributions.normal.Normal(torch.zeros(min(minibatch_size,mean_val.size(0)),hidden_size).cuda(),\\\n","                                                   torch.ones(min(minibatch_size,mean_val.size(0)),hidden_size).cuda())\n","      displaced = torch.distributions.normal.Normal(mean_val, sigma_val)\n","\n","      #sample z x 200 from displaced distribution(mean and s.d. from encoder run on data)\n","      #sample = displaced.sample(sample_shape=torch.Size())\n","      samples = [displaced.sample(sample_shape=torch.Size()) for i in range(K)]\n","\n","      #p and q are (200, 128,100)\n","      #density values for each samples, for each minibatch, for each uncorrelated z_i\n","      log_p_samples = torch.stack([standard.log_prob(samples[i].data) for i in range(len(samples))])\n","      log_q_samples = torch.stack([displaced.log_prob(samples[i].data) for i in range(len(samples))])\n","\n","      #torch.Size([200, 128, 784]) reconstructions of each sample/minibatch/image\n","      reconstructions = torch.stack([model.decode(samples[i].data).view(-1,image_size) for i in range(K)])\n","\n","      #p of x given z computed with Binary cross entropy ################# REVIEW DIMENSIONS HERE #####################\n","      log_p_x_given_z = ((images.view(-1,image_size) * torch.log(torch.clamp(reconstructions.data, 1e-9, 1))) + \\\n","                         ((1.0-images.view(-1,image_size))*torch.log(torch.clamp(1.0-reconstructions.data, 1e-9, 1))))\n","\n","      log_px.append( (-np.log(K) + torch.logsumexp(torch.sum(log_p_x_given_z, dim=-1) + \\\n","                                                   torch.sum(log_p_samples, dim=-1) - torch.sum(log_q_samples, dim=-1), 0)))\n","\n","    return torch.cat(log_px)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"oL5ukEKLHKmC","colab_type":"code","outputId":"ee42b096-fdfd-4f81-e847-a3da7668c743","executionInfo":{"status":"ok","timestamp":1555982960027,"user_tz":240,"elapsed":185552,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["#Evaluating log-likelihood with Variational Autoencoders 2.2\n","\n","valid_log_px = importance_sampling(model, valid_loader, 128, 784, 100, 200)\n","test_log_px = importance_sampling(model, test_loader, 128, 784, 100, 200)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"],"name":"stderr"}]},{"metadata":{"id":"o8NM7YJJPBa_","colab_type":"code","outputId":"0f8674b5-a946-406d-8acf-1bdc0d907187","executionInfo":{"status":"ok","timestamp":1555983132223,"user_tz":240,"elapsed":353,"user":{"displayName":"Andrew Williams","photoUrl":"","userId":"08053022487389580506"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"cell_type":"code","source":["print(\"ELBOs of VAE:\")\n","print(valid_elbo,' for the validation set')\n","print(test_elbo, ' for the test set\\n')\n","\n","print('Log-likelihood Estimates')\n","print(torch.mean(valid_log_px).cpu().numpy(),' for the validation set')\n","print(torch.mean(test_log_px).cpu().numpy(),' for the test set\\n')\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["ELBOs of VAE:\n","-95.90588443603515  for the validation set\n","-95.14183071289062  for the test set\n","\n","Log-likelihood Estimates\n","-90.28292  for the validation set\n","-89.59026  for the test set\n","\n"],"name":"stdout"}]},{"metadata":{"id":"GRH8Wjz1PLK3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}